{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the relative local folder where the data is stored\n",
    "base_fpath = os.getcwd() #the file path to the working directory of the code\n",
    "d_path = base_fpath.replace('notebooks', 'data') #raw data file path\n",
    "\n",
    "s_data = pd.read_csv(d_path + '/gas_data_cleaned.csv') #sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings from the last notebook we will want to create multiple dataframes, each capable of being ran in the future models we create. Each data frame will have their own pros and cons, which will be discussed during their creation. However, lets first start off by spliting the data into 3 sets: train, test, and future.\n",
    "The training dataset will be used to create a scaling method and to build our models.\n",
    "The test dataset will be used to verify our model and provide metrics on goodness of fit.\n",
    "The last dataset, arbitrarily named future, will consist of the data that was captured as part of later batches (i.e. at later dates). The future dataset will be used to evaluate the model's goodness of fit over time and help us determine if any of the sensors lose accuracy in measurement over time (often called drift)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to decide is which batches we should use for training and testing, and which ones should be used for all other data. I'm sure there is a good matematical guideline for determining how many data points are needed, but lets first look at how many data points are in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of Runs</th>\n",
       "      <th>% of Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>445</td>\n",
       "      <td>3.199137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244</td>\n",
       "      <td>8.943206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1586</td>\n",
       "      <td>11.401869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161</td>\n",
       "      <td>1.157441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>197</td>\n",
       "      <td>1.416247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2300</td>\n",
       "      <td>16.534867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3613</td>\n",
       "      <td>25.974119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>294</td>\n",
       "      <td>2.113587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>470</td>\n",
       "      <td>3.378864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3600</td>\n",
       "      <td>25.880661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    # of Runs  % of Data\n",
       "1         445   3.199137\n",
       "2        1244   8.943206\n",
       "3        1586  11.401869\n",
       "4         161   1.157441\n",
       "5         197   1.416247\n",
       "6        2300  16.534867\n",
       "7        3613  25.974119\n",
       "8         294   2.113587\n",
       "9         470   3.378864\n",
       "10       3600  25.880661"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = pd.DataFrame(s_data['BatchNumber'].value_counts(sort=False).sort_index())\n",
    "batch_data.rename(columns= {'BatchNumber': '# of Runs'}, inplace=True)\n",
    "batch_data['% of Data'] = batch_data['# of Runs'] / len(s_data) * 100\n",
    "batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five batches represent approximately 25% of the data or 3633 runs. If we were keeping the same number of dimensions that we currently have it would lead to a sparsely populated dimensional space. However, since we will be reducing the feature space, I think this is an acceptable number of data points to build and validate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent datasets from concentration split and chemical code split match so can keep just one X_train and X_test\n"
     ]
    }
   ],
   "source": [
    "ind_vars = tt_data.drop(columns=['ChemicalCode', 'Concentration', 'BatchNumber']).columns #column names of the independent variables\n",
    "tt_data = s_data[s_data['BatchNumber'] <= 5] #train test dataset\n",
    "X_tt = tt_data[ind_vars] #train test independent variable dataset\n",
    "y_tt_code = tt_data['ChemicalCode'] #1st dependent variable\n",
    "y_tt_con = tt_data['Concentration'] #2nd dependent variable\n",
    "\n",
    "X_train, X_test, y_code_train, y_code_test = train_test_split(X_tt, y_tt_code, test_size=.3, random_state=1991) #split the data 1st time\n",
    "X_train2, X_test2, y_con_train, y_con_test = train_test_split(X_tt, y_tt_con, test_size=.3, random_state=1991) #split the data for the second dependent variable, should be the same\n",
    "if (X_train['DR_1'] != X_train2['DR_1']).sum() == 0:\n",
    "    print('Independent datasets from concentration split and chemical code split match so can keep just one X_train and X_test')\n",
    "else:\n",
    "    print('Splits do not match so need to keep X_train, X_train2, X_test, and X_test2')\n",
    "\n",
    "\n",
    "f_data = s_data[s_data['BatchNumber'] > 5] #future dataset\n",
    "X_f = f_data[ind_vars] #train test independent variable dataset\n",
    "y_f_code = f_data['ChemicalCode'] #1st dependent variable\n",
    "y_f_con = f_data['Concentration'] #2nd dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets scale the data by stanardizing it such that the mean is 0 and the standard deviation is 1. I believe good practice dictates that we fit on the training data and transform the others based on that fitted scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_test_s = ss.transform(X_test)\n",
    "X_f_s = ss.transform(X_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engingeering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly Correlated Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
